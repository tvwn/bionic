{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f986aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Read the Energy and Weather datasets from CSV files\n",
    "energy_dataset = pd.read_csv(r'C:\\Users\\torst\\anaconda3\\O4\\Datat\\Energy and weather datasets\\energy_dataset.csv')\n",
    "weather_dataset = pd.read_csv(r'C:\\Users\\torst\\anaconda3\\O4\\Datat\\Energy and weather datasets\\weather_features.csv')\n",
    "\n",
    "# Remove unnecessary weather features\n",
    "weather_features_to_drop = ['weather_main', 'weather_description', 'pressure', 'rain_1h', 'rain_3h', 'snow_3h', 'weather_id']\n",
    "weather_dataset.drop(weather_features_to_drop, inplace=True, axis=1)\n",
    "\n",
    "# Remove specific energy generation types\n",
    "energy_generation_to_drop = ['generation fossil coal-derived gas', 'generation fossil peat', 'generation geothermal', \n",
    "                             'generation fossil oil shale', 'forecast wind offshore eday ahead', \n",
    "                             'generation hydro pumped storage aggregated', 'generation marine', \n",
    "                             'generation wind offshore']\n",
    "energy_dataset.drop(energy_generation_to_drop, inplace=True, axis=1)\n",
    "\n",
    "# Calculate the mean of weather data for each timestamp\n",
    "weather_dataset_mean = weather_dataset.groupby('dt_iso', as_index=False).mean()\n",
    "weather_dataset_mean = weather_dataset_mean.rename(columns={'dt_iso': 'time'})\n",
    "\n",
    "# Merge the energy and weather datasets based on their datetime index\n",
    "merged_dataset = pd.merge(energy_dataset, weather_dataset_mean, left_index=False, right_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e8b277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked correlation matrix for merged dataset:\n",
      "price day ahead                                1.000000\n",
      "price actual                                   0.732155\n",
      "generation fossil hard coal                    0.671596\n",
      "generation fossil gas                          0.640895\n",
      "generation hydro pumped storage consumption    0.600460\n",
      "generation fossil brown coal/lignite           0.567905\n",
      "total load forecast                            0.474649\n",
      "total load actual                              0.473869\n",
      "forecast wind onshore day ahead                0.428874\n",
      "generation other renewable                     0.428078\n",
      "generation wind onshore                        0.424899\n",
      "generation waste                               0.368036\n",
      "generation hydro run-of-river and poundage     0.294718\n",
      "generation fossil oil                          0.292793\n",
      "wind_deg                                       0.161827\n",
      "wind_speed                                     0.124252\n",
      "generation biomass                             0.108945\n",
      "temp_min                                       0.084635\n",
      "temp                                           0.070386\n",
      "forecast solar day ahead                       0.062118\n",
      "generation solar                               0.058392\n",
      "temp_max                                       0.050874\n",
      "generation nuclear                             0.044189\n",
      "generation other                               0.043599\n",
      "humidity                                       0.040554\n",
      "clouds_all                                     0.033799\n",
      "generation hydro water reservoir               0.017807\n",
      "Name: price day ahead, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fill missing values with 0\n",
    "merged_dataset = merged_dataset.fillna(0)\n",
    "\n",
    "# Calculate price discrepancy\n",
    "price_actual = merged_dataset['price actual'].values\n",
    "price_day_ahead = merged_dataset['price day ahead'].values\n",
    "price_discrep = abs(price_actual - price_day_ahead)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "merged_dataset.drop(['price actual', 'price day ahead'], inplace=True, axis=1)\n",
    "time_stamps = merged_dataset['time']\n",
    "merged_dataset.drop(['time', 'generation hydro pumped storage consumption',\n",
    "                     'generation fossil brown coal/lignite'], inplace=True, axis=1)\n",
    "\n",
    "# List of relevant generation features\n",
    "generationList = [\"generation biomass\", \"generation fossil gas\", \"generation fossil hard coal\",\n",
    "                  \"generation fossil oil\", \"generation hydro run-of-river and poundage\",\n",
    "                  \"generation hydro water reservoir\", \"generation nuclear\", \"generation other\",\n",
    "                  \"generation other renewable\", \"generation solar\", \"generation waste\",\n",
    "                  \"generation wind onshore\", \"total load actual\", \"temp\", \"temp_min\", \"temp_max\",\n",
    "                  \"humidity\", \"wind_speed\", \"wind_deg\", \"clouds_all\"]\n",
    "\n",
    "# Extract relevant features\n",
    "newDataset = merged_dataset[generationList]\n",
    "\n",
    "# Drop irrelevant features\n",
    "merged_dataset.drop(generationList, inplace=True, axis=1)\n",
    "\n",
    "# Shift data for 24 hours ago\n",
    "newDataset_24h_ago = newDataset.shift(-24)\n",
    "merged_dataset['time'] = time_stamps\n",
    "newDataset_24h_ago['time'] = time_stamps\n",
    "\n",
    "# Merge shifted dataset with current dataset\n",
    "merged_24_dataset = pd.merge(newDataset_24h_ago, merged_dataset, left_index=True, right_index=True)\n",
    "merged_24_dataset = merged_24_dataset[:-24]\n",
    "merged_24_dataset.drop(\"time\", inplace=True, axis=1)\n",
    "\n",
    "# Visualize histogram\n",
    "merged_24_dataset.hist(bins=100, figsize=(15, 15))\n",
    "plt.show()\n",
    "\n",
    "# Remove corresponding entries from price_discrep\n",
    "price_discrep = price_discrep[:-24]\n",
    "\n",
    "# Split the data for training and validation\n",
    "X = merged_24_dataset\n",
    "y = price_discrep\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, shuffle=True, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "degrees = [1, 2]\n",
    "\n",
    "for degree in degrees:\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "        (\"polynomial_features\", polynomial_features),\n",
    "        (\"linear_regression\", linear_regression)\n",
    "    ])\n",
    "    X_train_poly = pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate using cross-validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    score_mean = scores.mean()\n",
    "    \n",
    "    print(f\"Degree={degree:4d}, Mean Score={score_mean:4.2f}, Polynomial Features: {polynomial_features}\")\n",
    "    for i, score in enumerate(scores):\n",
    "        print(f\"CV fold {i}  =>  Score = {score:.2}\")\n",
    "\n",
    "# Set up preprocessing pipeline for polynomial regression\n",
    "poly_scaler = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "# Define function for training\n",
    "def Train(X_train, y_train, X_val, y_val, n_epochs, verbose=False):\n",
    "    print(f\"Training... n_epochs={n_epochs}\")\n",
    "\n",
    "    train_errors, val_errors = [], []\n",
    "\n",
    "    sgd_reg = SGDRegressor(\n",
    "        max_iter=10,\n",
    "        penalty=None,\n",
    "        eta0=0.005,\n",
    "        warm_start=True,\n",
    "        early_stopping=False,\n",
    "        learning_rate=\"constant\",\n",
    "        tol=-float(\"inf\"),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sgd_reg.fit(X_train, y_train)\n",
    "\n",
    "        y_train_predict = sgd_reg.predict(X_train)\n",
    "        y_val_predict = sgd_reg.predict(X_val)\n",
    "\n",
    "        mse_train = mean_squared_error(y_train, y_train_predict)\n",
    "        mse_val = mean_squared_error(y_val, y_val_predict)\n",
    "\n",
    "        train_errors.append(mse_train)\n",
    "        val_errors.append(mse_val)\n",
    "        if verbose:\n",
    "            print(f\"Epoch={epoch:4d}, MSE Train={mse_train:4.2f}, MSE Validation={mse_val:4.2f}\")\n",
    "\n",
    "    return train_errors, val_errors\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = Train(X_train_poly_scaled, y_train, X_val_poly_scaled, y_val, n_epochs, True)\n",
    "\n",
    "# Visualize histogram of the weather dataset\n",
    "weather_dataset.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = energy_dataset.corr()\n",
    "sorted_corr = corr_matrix[\"price day ahead\"].sort_values(ascending=False)\n",
    "\n",
    "# Set 'time' column as index and merge datasets\n",
    "energy_dataset.set_index('time', inplace=True)\n",
    "weather_dataset.set_index('dt_iso', inplace=True)\n",
    "merged_dataset = pd.merge(energy_dataset, weather_dataset, left_index=True, right_index=True)\n",
    "merged_dataset.reset_index(inplace=True)\n",
    "\n",
    "# Display first few rows of datasets\n",
    "energy_dataset.head()\n",
    "weather_dataset.head()\n",
    "\n",
    "# Visualize histogram of weather dataset\n",
    "weather_dataset.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "# Downsample weather dataset and merge\n",
    "downsampled_weather = weather_dataset.iloc[::5, :]\n",
    "downsampled_merged_dataset = pd.merge(energy_dataset, downsampled_weather, left_index=True, right_index=True)\n",
    "downsampled_merged_dataset.reset_index(inplace=True)\n",
    "\n",
    "# Compute correlation matrix for downsampled dataset\n",
    "corr_matrix_downsampled = downsampled_merged_dataset.corr()\n",
    "sorted_corr_downsampled = corr_matrix_downsampled[\"price day ahead\"].sort_values(ascending=False)\n",
    "downsampled_merged_dataset.describe()\n",
    "\n",
    "# Compute correlation matrix for merged dataset\n",
    "corr_matrix_merged = merged_dataset.corr()\n",
    "sorted_corr_merged = corr_matrix_merged[\"price day ahead\"].sort_values(ascending=False)\n",
    "merged_dataset.describe()\n",
    "\n",
    "# Visualize histogram of downsampled merged dataset\n",
    "downsampled_merged_dataset.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
